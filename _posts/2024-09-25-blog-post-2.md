---
Text: 
title: Statistic Learning Intro
date: 2024-09-25
permalink: /posts/2024/09/blog-post-2/
tags:
  - Statistics
  - AI
---
# 统计学习概论

## 监督学习
监督学习：从标注数据中学习预测模型的机器学习文同。标注数据表示输入输出的对应关系，预测模型对给定的输入产生相应的输出。监督学习的本质是学习输入到输出的统计规律。
> [!NOTE]
> 所谓的输入到输出的统计规律就是，我们给出许多对应的关系 `x<-->y`  座位我们从两个空间中的抽样。对于这样的抽样，我们转化为 $\mathcal P(X=x)$ 这样的一个概率测度，从而转化为探索一个分布的这样的过程

他有下面的一些基本元素
1. 输入空间、特征空间和输出空间
字面意思,所有输入的可能性构成了输入空间,所有输出的可能性构成了输出空间.他们的大小是不一定的.
一般来说每个输入是一个实例,通常通过特征向量来表示.(这里的“特征”二字的含义使用数字来抽象事物的特征,而不是代数学中的特征向量的感觉) 输入空间与特征空间并不总是一致的,很多时候,我们会通过随机变量来实现输入空间与特征空间的转换.一些默认的记号是
- $$X$$ 输入值(特征向量)
- $$Y$$ 输出
- $$T=\{(x_{1},y_{1}),\dots\}$$ 训练集
2. 联合概率分布
监督学习过程中我们一般假设输入与输出是随机变量 $$X、Y$$ 并且他们遵循联合概率分布 $$P(X,Y)$$ . $$P(X,Y)$$ 一般作为一个分布函数或者是分布密度函数.注意在学习过程中,我们通常是假定这样的分布是存在的,但是分布是什么样的则通常是一个未知的情况.
> 事实上,在概率论的学习过程中我们也知道,对于一个多元分布,很难得到它的分布形式
训练集和测试集一般我们看作是依照 $$P(X,Y)$$ 产生的.我们假设了这些数据是有一定的统计规律的

3. 假设空间
监督学习的目的是找到一个由输入到输出的映射,这一映射一般来说由模型来表示.换句话说,学习的目的就在于找到最好的模型.假设空间就是所有可能的映射构成的空间
监督学习的模型可以是概率模型($$P(Y|X)$$)或者是非概率模型$$Y=f(X)$$
4. 问题的形式化
![[Pasted image 20240920105416.png]]

一般来说,我们通过学习系统得到一个直接的输出 *决策函数* 或者是一个概率模型 *条件概率分布*.预测过程中,预测系统基于给定的输出 $x_{N+1}$
由模型 $$y_{N+1}=arg \max_{y} \hat{P(y|x_{N+1})}$$ 或者 $$y_{N+1}=\hat{f(x_{N+1})}$$,给出一个输出.学习系统试图通过训练集中的样本带来的信息来学习模型(如何利用我们暂时不提及)通过足够的学习,使得模型由充足的预测能力的时候样本输出与模型输出之间的差距就应该充分的小,这时候就可以是我们可以对未知的数据集有良好的预测推广.
## 无监督学习
无监督学习是指从无标注数据中学习预测模型的机器学习问题.无标注数据是自然得到的数据,预测模型表示数据的类别、转换或概率.无监督学习的本质是探索学习数据中存在的统计规律或潜在结构.
同样的,模型的输入与输出的所有可能取值的集合分别称为输入空间与输出空间.输入一般是通过数字实现对与数据的特征的展示,每一个输出则是对输入的分析结果,包括了输入的类别、转换或概率表示.模型可以实现对数据的 *聚类、将维或概率估计*.
假设 $$\mathcal X$$ 是一输入空间, $$\mathcal Z$$ 是隐式结构空间,那么我们的学习模型和之前的监督学习一样,可以是概率模型或者非概率模型,只不过此时的输出不再是一个数值,而是一个模型(或者是模型的参数,这取决于我们对结构空间的规定).
包含了所有可能模型的集合称为假设空间.无监督学习旨在从假设空间中选出在给定评价标准下的最有模型.
无监督学习的一般范式和监督学习是差不多的.无论是概率模型还是非概率模型,,对于给定的输入,我们都可以得到一个对应的输出,之后进行聚类或者将为.或者是,利用概率模型直接去估计概率
无监督学习一般由两类作用
- 对已有数据的分析
- 对未来数据的预测

本课程我们一般注重于上述两类模型的学习
## 模型的学习划分
1. 概率模型与非概率模型
之前我们一直在谈概率模型与非概率模型.
监督学习中,概率模型一般的形式是

$$
P(y|x)
$$

的条件概率,非概率模型则是

$$
y=f(x)
$$

无监督学习中,概率模型去体哦阿健概率分布形式 $P(z|x)$ 或 $P(x|z)$ ,非概率模型取函数形式取函数形式$z=g(x)$ ***监督学习中的概率模型是生成模型、非概率模型是判别模型***

本书会介绍
- 决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、概率潜在语义分析、潜在dilikelei分配、高斯混合模型是***概率模型***
- 感知机、支持向量机、$k$ 近邻、AdaBoost、$k$ 均值、潜在语义分析、神经网络则是非概率模型
- 逻辑斯蒂回归则两者都有
而事实上,条件概率分布与函数之间可以通过最大化或归一化相互进行转换.所以,概率模型和非概率模型的区别不在于输入与输出之间的映射关系,而在于模型的内在结构.
>概率模型一定可以表示为联合概率分布的形式.其中的变量表示输入、输出、隐变量甚至参数.而针对非概率模型这样的联合概率分布则是不一定存在的.

概率模型的代表是 ***概率图*** 模型,由有向图或者无向图比哦阿是的概率模型.而联合分布概率可以根据图的结构分解为因子乘积的形式.贝叶斯网络、马尔可夫随机场、条件随机场都是概率图模型

2. 线性模型与非线性模型
![[统计学习方法.pdf|统计学习方法, p.35]]
3. 参数化模型与非参数化模型
![[统计学习方法.pdf|统计学习方法, p.35]]
 ## 在线学习与批量学习
 二者的区别在于每次接受的样本量不同.在线学习每次接受一个样本进行预测之后学习模型,并且不断的重复这一过程.
 但是批量学习则一次就接受所有的数据学习模型,之后进行预测.
 利用梯度下降的感知机学习算法就是在线学习算法
## 贝叶斯学习
贝叶斯学习的主要想法是,利用贝叶斯定理,计算在给定数据条件下模型的条件概率(后验概率),并应用这个原理进行模型的估计.我们将学习包括朴素贝叶斯、潜在狄利克雷分配在哪的贝叶斯学习
首先我我们考虑贝叶斯公示
假设随机变量 $D$ 表示数据,随机变量 $\theta$ 表示模型参数,从而我们知道

$$
P(\theta|D) = \frac{P(\theta)P(D|\theta)}{P(D)}
$$

其中的 $P(\theta)$ 是我们的先验概率,$P(D|\theta)$ 则是似然函数.我们主观的认为我们的参数服从某个参数,而 $P(D|\theta)$ 则是我们基于样本数据可以得到的一个似然函数
模型估计的时候,估计整个后验概率分布$P(\theta|D)$ .如果需要给出一个模型,我们通常取后眼概率最大的模型.
实际预测上,我们计算数据对后验概率分布的期望值

$$
P(x|D)=\int P(x|\theta,D) P(\theta|D) d\theta
$$

或者说,计算

$$
\hat\theta = \int \theta P(\theta|D)d\theta
$$

极大似然估计是基于对似然函数取得最大值的一种估计,而贝叶斯估计则是一种基于先验估计对与最大似然函数的后验估计.利用贝叶斯估计我们得到了参数的一个分布,基于这样的分布,我们通过求期望的方式获得最佳参数
## 核方法
Kernel Method 使用核函数表示和学习非线性模型的一种机器学习方法,可以用于监督学习和无监督学习.把一些线性模型的学习方法基于相似度的计算,更加具体的,向量内急计算.核方法可以把他们扩展到非线性模型的学习,使其应用范围更加的广泛
常见的核方法包括了 SVM、核PCA、核K均值
# 生成模型与判别模型
监督学习的任务是学习一个模型,利用这样的模型,对于给定的输入预测相应的输出.这样的模型一般可以是决策函数

$$
Y = f(X)
$$

或者是一个条件概率分布

$$
P(Y|X)
$$

监督学习方法有可以氛围生成方法和判别方法
生成方法由数据学习联合概率分布
$P(X,Y)$
, 然后求出条件概率分布 
$P(Y|X)$
作为预测的模型,也就是一个生成模型

$$
P(Y|X) = \frac{P(X,Y)}{P(X)}
$$

上述模型展示了给定输入 $X$ 的时候,产生 $Y$ 的生成关系.也就是说我们仅仅依靠对联合分布 $(X,Y)$ 的样本进行学习,假定出了一个模型,基于这样的模型,基于对$X$分布对假设,我们可以构建出一个给定 $X$ 对应对 $Y$ 的一个条件分布.至于学习的方法,常见的有朴素贝叶斯和隐马尔可夫模型,我们会在后续进行描述
判别方法则是利用数据直接的学习 $f(X)$ 或者是 $P(Y|X)$ 从而来作为一个预测的模型,也就是判别模型.判别方法关系的是对于给定的输入 $X$, 应该预测怎么样的输出 $Y$.

| 方法   |        |               |               |
| ---- | ------ | ------------- | ------------- |
| 生成方法 | 快      | 样本容量增加的时候快速收敛 | 存在隐变量时候依旧可以学习 |
| 判别方法 | 学习准确率高 | 方便简化学习        |               |

